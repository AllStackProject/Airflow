from airflow import DAG
from airflow.decorators import task
from datetime import datetime
import boto3
import os
import subprocess
import shutil
import requests
from kubernetes.client import models as k8s

# -------------------------------
# ê¸°ë³¸ ì„¤ì •
# -------------------------------
BUCKET_ORIGINAL = "privideo-original"
BUCKET_OUTPUT = "privideo-output"
OUTPUT_DIR = "/workspace"
RESOLUTIONS = ["360", "540", "720"]


# -------------------------------
# ë¦¬ì†ŒìŠ¤ ì»¨í…Œì´ë„ˆ ìƒì„±
# -------------------------------
def make_container(cpu_req, cpu_limit, mem_req, mem_limit):
    return k8s.V1Container(
        name="base",
        image="leeyonghun/airflow-ffmpeg:v4",
        env_from=[
            k8s.V1EnvFromSource(secret_ref=k8s.V1SecretEnvSource(name="airflow-aws"))
        ],
        resources=k8s.V1ResourceRequirements(
            requests={"cpu": cpu_req, "memory": mem_req},
            limits={"cpu": cpu_limit, "memory": mem_limit},
        ),
    )


# í•´ìƒë„ë³„ ë¦¬ì†ŒìŠ¤ ë§¤í•‘
def get_transcode_container(res):
    if res == "360":
        return make_container("1000m", "1000m", "1Gi", "2Gi")
    elif res == "540":
        return make_container("1000m", "2000m", "1Gi", "3Gi")
    elif res == "720":
        return make_container("1000m", "3000m", "1Gi", "4Gi")
    else:
        raise ValueError("Unsupported resolution")


# íŒ¨í‚¤ì§• ì»¨í…Œì´ë„ˆ (ì €ì‚¬ì–‘)
package_container = make_container("500m", "1000m", "1Gi", "2Gi")


# -------------------------------
# PVC ë§ˆìš´íŠ¸ Pod Override
# -------------------------------
def exec_config(container):
    return {
        "pod_override": k8s.V1Pod(
            spec=k8s.V1PodSpec(
                tolerations=[
                    k8s.V1Toleration(
                        key="role",
                        operator="Equal",
                        value="airflow-worker",
                        effect="NoSchedule"
                    )
                ],
                node_selector={
                    "role": "airflow-worker"
                },
                containers=[
                    k8s.V1Container(
                        name=container.name,
                        image=container.image,
                        env_from=container.env_from,
                        resources=container.resources,
                        volume_mounts=[
                            k8s.V1VolumeMount(
                                name="worker-temp",
                                mount_path="/workspace"
                            )
                        ],
                    )
                ],
                volumes=[
                    k8s.V1Volume(
                        name="worker-temp",
                        persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                            claim_name="pvc-hdd-airflow-worker-temp"
                        )
                    )
                ],
                restart_policy="Never",
            )
        )
    }

# -------------------------------
# API ì½œë°± í•¨ìˆ˜
# -------------------------------

def dag_fail_callback(context):
    dag_id = context['dag'].dag_id
    org_id = context['dag_run'].conf.get("org_id")
    video_uuid = context['dag_run'].conf.get("video_uuid")
    error = str(context.get("exception"))

    url = f"https://privideo-backend-service.web.svc.cluster.local/{org_id}/video/airflow/status"

    requests.post(
        url,
        json={
            "dag_id": dag_id,
            "video_uuid": video_uuid,
            "status": "FAILED",
            "message": "[DAG Failed] " + error
        },
        timeout=3
    )


def dag_success_callback(context):
    dag_id = context['dag'].dag_id
    org_id = context['dag_run'].conf.get("org_id")
    video_uuid = context['dag_run'].conf.get("video_uuid")

    url = f"https://privideo-backend-service.web.svc.cluster.local/{org_id}/video/airflow/status"

    requests.post(
        url,
        json={
            "dag_id": dag_id,
            "video_uuid": video_uuid,
            "status": "SUCCESS",
            "message": "[DAG Success] Success upload"
        },
        timeout=3
    )


def task_fail_callback(context):
    dag_id = context['dag'].dag_id
    task_id = context['task_instance'].task_id
    org_id = context['dag_run'].conf.get("org_id")
    video_uuid = context['dag_run'].conf.get("video_uuid")

    url = f"https://privideo-backend-service.web.svc.cluster.local/{org_id}/video/airflow/status"

    requests.post(
        url,
        json={
            "dag_id": dag_id,
            "video_uuid": video_uuid,
            "status": "FAILED",
            "message": "[Task Failed] Failed on " + task_id
        },
        timeout=3
    )
    
# -------------------------------
# 1) ë‹¤ìš´ë¡œë“œ
# -------------------------------
@task(executor_config=exec_config(package_container))
def download_video(org_id: int, video_uuid: str):

    s3 = boto3.client("s3")
    s3_key = f"org-{org_id}/{video_uuid}/original.mp4"
    local_input = f"{OUTPUT_DIR}/{video_uuid}_original.mp4"

    print(f"â¬‡ï¸ Download â†’ s3://{BUCKET_ORIGINAL}/{s3_key}")
    s3.download_file(BUCKET_ORIGINAL, s3_key, local_input)

    return local_input


# -------------------------------
# 2) íŠ¸ëžœìŠ¤ì½”ë”© (í•´ìƒë„ë³„ ë³‘ë ¬)
# -------------------------------
def build_transcode_task(resolution):

    container = get_transcode_container(resolution)

    @task(
        task_id=f"transcode_video_{resolution}p",
        executor_config=exec_config(container)
    )
    def _transcode(local_input: str, org_id: int, video_uuid: str):

        output_local = f"{OUTPUT_DIR}/{video_uuid}_{resolution}p.mp4"

        cmd = (
            f"ffmpeg -y -i {local_input} "
            f"-vf scale=-2:{resolution} "
            f"-c:v libx264 -preset veryfast -c:a aac {output_local}"
        )

        print(f"ðŸŽ¬ Transcoding {resolution}p â†’ {output_local}")
        subprocess.run(cmd, shell=True, check=True)

        s3 = boto3.client("s3")
        key = f"org-{org_id}/{video_uuid}/{resolution}p.mp4"

        print(f"â¬†ï¸ Upload {output_local} â†’ s3://{BUCKET_ORIGINAL}/{key}")
        s3.upload_file(output_local, BUCKET_ORIGINAL, key)

        return output_local

    return _transcode


# -------------------------------
# 3) íŒ¨í‚¤ì§• + ì—…ë¡œë“œ
# -------------------------------
@task(executor_config=exec_config(package_container))
def packaging_and_upload(org_id: int, video_uuid: str, trans_outputs: list):

    s3 = boto3.client("s3")

    out_dir = f"{OUTPUT_DIR}/hls_{video_uuid}"
    os.makedirs(out_dir, exist_ok=True)

    rendition_infos = []

    for mp4_path in trans_outputs:
        res = mp4_path.split("_")[-1].replace("p.mp4", "")
        res_dir = f"{out_dir}/{res}p"
        os.makedirs(res_dir, exist_ok=True)

        cmd = (
            f"ffmpeg -i {mp4_path} -c copy "
            f"-map 0 -f hls -hls_time 10 -hls_playlist_type vod "
            f"-hls_segment_filename '{res_dir}/segment%03d.ts' "
            f"{res_dir}/index.m3u8"
        )

        print(f"ðŸ“¦ Packaging {res}p â†’ {res_dir}")
        subprocess.run(cmd, shell=True, check=True)

        rendition_infos.append((res, f"{res}p/index.m3u8"))

    # MASTER M3U8 ìƒì„±
    master_path = f"{out_dir}/master.m3u8"
    with open(master_path, "w") as m:
        m.write("#EXTM3U\n")
        for res, playlist in rendition_infos:
            bandwidth = int(res) * 1000
            m.write(
                f"#EXT-X-STREAM-INF:BANDWIDTH={bandwidth},RESOLUTION=1920x{res}\n"
                f"{playlist}\n"
            )

    # ì „ì²´ ì—…ë¡œë“œ (privideo-output)
    print("â¬†ï¸ Uploading all HLS outputs to S3...")

    for root, dirs, files in os.walk(out_dir):
        for file in files:
            local_path = os.path.join(root, file)
            key = f"hls/org-{org_id}/{video_uuid}/{local_path.replace(out_dir, '').lstrip('/')}"
            print(f"S3 â†’ {key}")
            s3.upload_file(local_path, BUCKET_OUTPUT, key)

    print("ðŸŽ‰ Packaging + Upload completed.")
    return True


# -------------------------------
# 4) PVC ì •ë¦¬(ì‚­ì œ)
# -------------------------------
@task(executor_config=exec_config(package_container))
def cleanup_local_files(video_uuid: str):

    base = OUTPUT_DIR
    print(f"ðŸ§¹ Cleaning up PVCâ€¦ {base}")

    # ì›ë³¸ + MP4
    for f in os.listdir(base):
        if f.startswith(video_uuid):
            path = os.path.join(base, f)
            print(f"ðŸ—‘ Removing {path}")
            if os.path.isdir(path):
                shutil.rmtree(path, ignore_errors=True)
            else:
                os.remove(path)

    # HLS ë””ë ‰í„°ë¦¬
    hls_dir = os.path.join(base, f"hls_{video_uuid}")
    if os.path.exists(hls_dir):
        print(f"ðŸ—‘ Removing HLS: {hls_dir}")
        shutil.rmtree(hls_dir, ignore_errors=True)

    print("ðŸ§¼ PVC cleanup complete.")
    return True


# -------------------------------
# DAG ì •ì˜ (ì½œë°± ì¶”ê°€ ì™„ë£Œ)
# -------------------------------
with DAG(
    dag_id="video_transcode_hls_pipeline",
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
    on_success_callback=dag_success_callback,   # DAG ì „ì²´ ì„±ê³µ ì‹œ
    on_failure_callback=dag_fail_callback,      # DAG ì „ì²´ ì‹¤íŒ¨ ì‹œ
    default_args={
        "on_failure_callback": task_fail_callback  # Task ì‹¤íŒ¨ ì‹œ
    }
) as dag:

    org_id = "{{ dag_run.conf['org_id'] }}"
    video_uuid = "{{ dag_run.conf['video_uuid'] }}"

    download = download_video(org_id, video_uuid)

    trans_tasks = [
        build_transcode_task(r)(download, org_id, video_uuid)
        for r in RESOLUTIONS
    ]

    upload = packaging_and_upload(org_id, video_uuid, trans_outputs=trans_tasks)

    clean = cleanup_local_files(video_uuid)

    download >> trans_tasks >> upload >> clean
